{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing with NLTK\n",
    "Date: August, 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTaSjTqAS52_"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "We are going to explore techniques to clean and convert text features into numerical features that machine learning algoritms can work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pF3iaKKTJ8f"
   },
   "source": [
    "# Common text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnyC13iMTVye"
   },
   "outputs": [],
   "source": [
    "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVbv9CjCTYd_"
   },
   "source": [
    "Let's first lowercase our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvImMVoCTfeJ",
    "outputId": "b7968d3b-0814-45e1-ab4d-3ada8bad0688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   this is a message to be cleaned. it may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwmJTvYjTok7"
   },
   "source": [
    "We can get rid of leading/trailing whitespace with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tgYvyLxZTvqG",
    "outputId": "03b1cd4c-d55e-4d70-f421-cd36196f9c94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned. it may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .\n"
     ]
    }
   ],
   "source": [
    "text = text.strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjVwksteT1pv"
   },
   "source": [
    "Remove HTML tags/markups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrSiYQxkT5OG",
    "outputId": "bf7b072f-82f1-4b2c-9009-f634f02d239c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned. it may involve some things like: , ?, :, ''  adjacent spaces and tabs     .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = re.compile('<.*?>').sub('', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYECQXj1UHWw"
   },
   "source": [
    "Replace punctuation with space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IyuvD9wHUOQ3",
    "outputId": "e1bc8b49-5b1d-49af-df62-29c940d455f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned  it may involve some things like              adjacent spaces and tabs      \n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFFlnXDnU2hU"
   },
   "source": [
    "Remove extra space and tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMPtVUzIU5Vo",
    "outputId": "5af84fc2-104e-44ae-c795-887774eae1b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned it may involve some things like adjacent spaces and tabs \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = re.sub('\\s+', ' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9444AtnBVEH7"
   },
   "source": [
    "# Lexicon based text processing\n",
    "\n",
    "Lexicon based methods are usually used to normalize sentences in our dataset.\n",
    "\n",
    "By **normalization**, here, we mean putting words in the sentences into a similar format that will enhance similarities (if any) between sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdJqHMRcVWvp"
   },
   "source": [
    "**Stop word removal**: There can be some words in our sentences that occur very frequently and don't contribute too much to the overall meaning of the sentences. We usually have a list of these words and remove them from each our sentences. For example: \"a\", \"an\", \"the\", \"this\", \"that\", \"is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzgpLlr1Vc_o",
    "outputId": "1e9b89df-0c57-432a-e8bd-9a0aaf4f3a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message be cleaned may involve some things like adjacent spaces tabs \n"
     ]
    }
   ],
   "source": [
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n",
    "\n",
    "filtered_sentence = []\n",
    "words = text.split(\" \")\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "text = \" \".join(filtered_sentence)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rETy_Xt4VtRD"
   },
   "source": [
    "Stemming: Stemming is a rule-based system to convert words into their root form.\n",
    "\n",
    "It removes suffixes from words. This helps us enhace similarities (if any) between sentences.\n",
    "\n",
    "Example:\n",
    "\n",
    "\n",
    "\n",
    "*   \"jumping\", \"jumped\" -> \"jump\"\n",
    "*   \"cars\" -> \"car\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1lBqfznV2Dc",
    "outputId": "a2c12e16-b74c-49d5-e16d-94738dadd84b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messag be clean may involv some thing like adjac space tab \n"
     ]
    }
   ],
   "source": [
    "# We use the NLTK library\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "stemmed_sentence = []\n",
    "words = text.split(\" \")\n",
    "for w in words:\n",
    "    stemmed_sentence.append(snow.stem(w))\n",
    "text = \" \".join(stemmed_sentence)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpUBMFw9WMRK"
   },
   "source": [
    "# Features extraction\n",
    "\n",
    "We assume we will first apply the common and lexicon based pre-processing to our text. After those, we will convert our text data into numerical data with the Bag of Words (BoW) representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t37hcj0HWWCH"
   },
   "source": [
    "**Bag of Words (BoW)**: A modeling technique to convert text information into numerical representation.\n",
    "Machine learning models expect numerical or categorical values as input and won't work with raw text data.\n",
    "\n",
    "Steps:\n",
    "1.   Create vocabulary of known words\n",
    "2.   Measure presence of the known words in sentences\n",
    "\n",
    "We will use the sklearn library's Bag of Words implementation:\n",
    "* **CountVectorizer:** Sklearn text vectorizer, converts a collection of text documents to a matrix of token counts\n",
    "* **TfidfVectorizer:** Sklearn text vectorizer, converts a collection of text documents to a matrix of TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKvWGu3DW9jY"
   },
   "outputs": [],
   "source": [
    "#We will use the sklearn library's Bag of Words implementation:\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "sentences = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document one one one one one one one one?'\n",
    "]\n",
    "countVectorizer = CountVectorizer(binary=True)\n",
    "X = countVectorizer.fit_transform(sentences)\n",
    "\n",
    "#Returns normalized term frequencies matrix when 'use_idf'=False\n",
    "tfVectorizer = TfidfVectorizer(use_idf=False)\n",
    "tf = tfVectorizer.fit_transform(sentences)\n",
    "\n",
    "#Returns smoother TF-IDF matrix when 'use_idf'=True\n",
    "tfIdfVectorizer = TfidfVectorizer(use_idf=True)\n",
    "tfIdf = tfIdfVectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzg1xhbTXGaj"
   },
   "source": [
    "Let's print the vocabulary below.\n",
    "\n",
    "* Each number next to a word shows the index of it in the vocabulary (From 0 to 8 here).\n",
    "\n",
    "* They are alphabetically ordered-> and:0, document:1, first:2, .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xMnk5nOXTJM",
    "outputId": "7f9d08cc-5ac6-4fbd-c752-d7bd957bb8fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      " {'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary: \\n',countVectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXdYnj_KXsjK",
    "outputId": "0b0328ce-6377-426c-cf75-cbab40730b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Binary Features: \n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 1 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 1 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print('Bag of Words Binary Features: \\n',X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_8K_VWuYco4"
   },
   "source": [
    "What happens when we encounter a new word during prediction?\n",
    "\n",
    "* New words will be skipped.\n",
    "* This usually happens when we are making predictions. For our test and validation data/text, we need to use the **.transform()** function this time.\n",
    "* This simulates a real-time prediction case where we cannot re-train the model quickly whenever we receive new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fq6qiC1VYnLr",
    "outputId": "8049a834-00f4-4f1d-e5ca-563498d9e707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\"this document has some new words\",\n",
    "                 \"this one is new too\"]\n",
    "count_vectors = countVectorizer.transform(test_sentences)\n",
    "print(count_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBLDhYvsZVnK"
   },
   "source": [
    "See that these last two vectors have the same lenght 9 (same vocabulary) like the ones before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F9o5diylsDR"
   },
   "source": [
    "Let's print the Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20c9lPV8lt5U",
    "outputId": "9b7cb3e3-82a1-498e-f503-1020fec2c618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features names: \n",
      " ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Shape: \n",
      " (4, 9)\n",
      "Vocabulary: \n",
      " {'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
      "TF for Sentence 0: \n",
      "                 TF\n",
      "document  0.447214\n",
      "first     0.447214\n",
      "is        0.447214\n",
      "the       0.447214\n",
      "this      0.447214\n",
      "and       0.000000\n",
      "one       0.000000\n",
      "second    0.000000\n",
      "third     0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('Features names: \\n',tfVectorizer.get_feature_names_out())\n",
    "print('Shape: \\n',tf.shape)\n",
    "print('Vocabulary: \\n',tfVectorizer.vocabulary_)\n",
    "\n",
    "df = pd.DataFrame(tf[0].T.todense(), index=tfVectorizer.get_feature_names_out(), columns=[\"TF\"])\n",
    "df = df.sort_values('TF', ascending=False)\n",
    "print('TF for Sentence 0: \\n',df.head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3iC1aIEmNV1"
   },
   "source": [
    "Let's print the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iEK2BWEmaau",
    "outputId": "4709f897-1781-4428-f5cd-a242ad0c7971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features names: \n",
      " ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Shape: \n",
      " (4, 9)\n",
      "Vocabulary: \n",
      " {'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
      "Inverse Document Frequency Vector: \n",
      " [1.91629073 1.22314355 1.51082562 1.22314355 1.51082562 1.91629073\n",
      " 1.         1.91629073 1.22314355]\n",
      "TF-IDF for Sentence 0: \n",
      "             TF-IDF\n",
      "first     0.541977\n",
      "document  0.438777\n",
      "is        0.438777\n",
      "this      0.438777\n",
      "the       0.358729\n",
      "and       0.000000\n",
      "one       0.000000\n",
      "second    0.000000\n",
      "third     0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('Features names: \\n',tfIdfVectorizer.get_feature_names_out())\n",
    "print('Shape: \\n',tfIdf.shape)\n",
    "print('Vocabulary: \\n',tfIdfVectorizer.vocabulary_)\n",
    "print('Inverse Document Frequency Vector: \\n',tfIdfVectorizer.idf_)\n",
    "\n",
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print('TF-IDF for Sentence 0: \\n',df.head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-UdtpIuXlS4"
   },
   "source": [
    "**Note:**\n",
    "* Sklearn automatically removes punctuation, but doesn't do the other extra pre-processing methods we discussed here.\n",
    "* Lexicon-based methods are also not automaticaly applied, we need to call those methods before feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9J6RaRRol-d"
   },
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URbOqMj6oxjh"
   },
   "source": [
    "Let's see all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5p60z8NkozzI"
   },
   "outputs": [],
   "source": [
    "# Prepare cleaning functions\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preProcessText(text):\n",
    "    # lowercase and strip leading/trailing white space\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # remove HTML tags\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "\n",
    "    # remove punctuation\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "\n",
    "    # remove extra white space\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def lexiconProcess(text, stop_words, stemmer):\n",
    "    filtered_sentence = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(stemmer.stem(w))\n",
    "    text = \" \".join(filtered_sentence)\n",
    "\n",
    "    return text\n",
    "\n",
    "def cleanSentence(text, stop_words, stemmer):\n",
    "    return lexiconProcess(preProcessText(text), stop_words, stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5bVu39uo7OQ"
   },
   "source": [
    "Prepare vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJFplkROo9SN"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "textvectorizer = CountVectorizer(binary=True)# can also limit vocabulary size here, with say, max_features=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or3xkjqspBCl"
   },
   "source": [
    "Clean and vectorize a text feature with four samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wzNwLdpnpKzS",
    "outputId": "08d4e39f-d173-4aa7-f8be-217b653fbbc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Vocabulary: \n",
      " {'like': 11, 'materi': 13, 'color': 4, 'overal': 19, 'how': 10, 'look': 12, 'work': 29, 'okay': 18, 'first': 7, 'two': 27, 'time': 26, 'use': 28, 'but': 3, 'third': 24, 'burn': 2, 'my': 15, 'face': 6, 'am': 1, 'not': 17, 'sure': 23, 'about': 0, 'product': 21, 'never': 16, 'thought': 25, 'would': 30, 'pay': 20, 'so': 22, 'much': 14, 'for': 8, 'hair': 9, 'dryer': 5}\n",
      "Bag of Words Binary Features: \n",
      " [[0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1]]\n",
      "Shape: \n",
      " (4, 31)\n"
     ]
    }
   ],
   "source": [
    "text_feature = [\"I liked the material, color and overall how it looks.<br /><br />\",\n",
    "             \"Worked okay first two times I used it, but third time burned my face.\",\n",
    "             \"I am not sure about this product.\",\n",
    "             \"I never thought I would pay so much for a hair dryer.\",\n",
    "            ]\n",
    "\n",
    "print(len(text_feature))\n",
    "\n",
    "# Clean up the text\n",
    "text_feature_cleaned = [cleanSentence(item, stop_words, stemmer) for item in text_feature]\n",
    "\n",
    "# Vectorize the cleaned text\n",
    "text_feature_vectorized = textvectorizer.fit_transform(text_feature_cleaned)\n",
    "print('Vocabulary: \\n', textvectorizer.vocabulary_)\n",
    "print('Bag of Words Binary Features: \\n', text_feature_vectorized.toarray())\n",
    "print('Shape: \\n',text_feature_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XemTlvN3ppa3"
   },
   "source": [
    "# Using the Natural Language Toolkit (NLTK) library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmleMzylg9PF"
   },
   "source": [
    "Importing all the resources to use the NLTK library in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvYrWQ1rg_Be",
    "outputId": "b51932d8-aa0f-429e-e7d9-62014f3a9afd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e61Inp_ft4d",
    "outputId": "3b1ed728-a8f3-42e3-b43d-dafd271089c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Junta Electoral de Yachay Tech, comunica que de acuerdo al cronograma establecido desde el 05 al 18 de agosto, se realizarán las inscripciones de candidaturas. #SomosYachayTech\n",
      "Yachay Tech recibió la visita de 40 estudiantes de la Universidad Estatal Península de Santa Elena, @UPSE_ec con el objetivo de conocer las instalaciones, laboratorios y equipos que dispone la universidad, además de la oferta académica vigente de posgrados.\n",
      "Con mucha alegría y entusiasmo el día de hoy, se realizó la socialización de los programas y proyectos de vinculación de la Escuela de Investigaciones Agropecuarias y Agroindustriales, contando con la participación de distintas comunidades. 1/2 #SomosYachayTech\n",
      "Además se realizó una firma de convenio y carta compromiso con @AgriculturaEc y la Fundacion @HeiferEcuador, con el objetivo de establecer mecanismos de cooperación interinstitucional y desarollo productivo. #SomosYachayTech\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"https://raw.githubusercontent.com/erickedu85/dataset/master/yt_tweets.json\")\n",
    "\n",
    "for text in df[0]:\n",
    "  print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ial15Nd7f5c9"
   },
   "source": [
    "Using the stop-words in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TP_siT-Lf7vV",
    "outputId": "267ae067-998c-4c1c-a5d4-2bc27da69576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopword_es = stopwords.words('spanish')\n",
    "print (stopword_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY1-AnDZhWdM"
   },
   "source": [
    "* **Word tokenization** is a method by which we break the whole paragraph into individual tokens of strings\n",
    "* **Stemming** is a process by which we tend to form the word stem out of the given word, for example, if the given word is ‘lately’, then the stemming will cut ‘ly’ and give the output as ‘late’, this is done in order to find more context for information retrieval and to reduce the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kkDYlQehXjn",
    "outputId": "e0ce567c-5fef-4660-d6b9-62d1bcf88957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      " ['La', 'Junta', 'Electoral', 'de', 'Yachay', 'Tech', ',', 'comunica', 'que', 'de', 'acuerdo', 'al', 'cronograma', 'establecido', 'desde', 'el', '05', 'al', '18', 'de', 'agosto', ',', 'se', 'realizarán', 'las', 'inscripciones', 'de', 'candidaturas', '.', '#', 'SomosYachayTech']\n",
      "Cleaned tokens and punctuation: \n",
      " ['la', 'junt', 'electoral', 'yachay', 'tech', 'comun', 'acuerd', 'cronogram', 'establec', '05', '18', 'agost', 'realiz', 'inscripcion', 'candidatur', 'somosyachaytech']\n",
      "Tagged tokens: \n",
      " [('la', 'NN'), ('junt', 'NN'), ('electoral', 'JJ'), ('yachay', 'NN'), ('tech', 'NN'), ('comun', 'NN'), ('acuerd', 'NN'), ('cronogram', 'NN'), ('establec', 'VBZ'), ('05', 'CD'), ('18', 'CD'), ('agost', 'NN'), ('realiz', 'NN'), ('inscripcion', 'NN'), ('candidatur', 'NN'), ('somosyachaytech', 'NN')]\n",
      "\n",
      "\n",
      "Tokens: \n",
      " ['Yachay', 'Tech', 'recibió', 'la', 'visita', 'de', '40', 'estudiantes', 'de', 'la', 'Universidad', 'Estatal', 'Península', 'de', 'Santa', 'Elena', ',', '@', 'UPSE_ec', 'con', 'el', 'objetivo', 'de', 'conocer', 'las', 'instalaciones', ',', 'laboratorios', 'y', 'equipos', 'que', 'dispone', 'la', 'universidad', ',', 'además', 'de', 'la', 'oferta', 'académica', 'vigente', 'de', 'posgrados', '.']\n",
      "Cleaned tokens and punctuation: \n",
      " ['yachay', 'tech', 'recib', 'visit', '40', 'estudi', 'univers', 'estatal', 'peninsul', 'sant', 'elen', 'upse_ec', 'objet', 'conoc', 'instal', 'laboratori', 'equip', 'dispon', 'univers', 'ademas', 'ofert', 'academ', 'vigent', 'posgr']\n",
      "Tagged tokens: \n",
      " [('yachay', 'NN'), ('tech', 'NN'), ('recib', 'VBP'), ('visit', 'NN'), ('40', 'CD'), ('estudi', 'NN'), ('univers', 'NNS'), ('estatal', 'VBP'), ('peninsul', 'NN'), ('sant', 'JJ'), ('elen', 'NN'), ('upse_ec', 'JJ'), ('objet', 'NN'), ('conoc', 'NN'), ('instal', 'JJ'), ('laboratori', 'NN'), ('equip', 'NN'), ('dispon', 'NN'), ('univers', 'NNS'), ('ademas', 'VBP'), ('ofert', 'JJ'), ('academ', 'JJ'), ('vigent', 'NN'), ('posgr', 'NN')]\n",
      "\n",
      "\n",
      "Tokens: \n",
      " ['Con', 'mucha', 'alegría', 'y', 'entusiasmo', 'el', 'día', 'de', 'hoy', ',', 'se', 'realizó', 'la', 'socialización', 'de', 'los', 'programas', 'y', 'proyectos', 'de', 'vinculación', 'de', 'la', 'Escuela', 'de', 'Investigaciones', 'Agropecuarias', 'y', 'Agroindustriales', ',', 'contando', 'con', 'la', 'participación', 'de', 'distintas', 'comunidades', '.', '1/2', '#', 'SomosYachayTech']\n",
      "Cleaned tokens and punctuation: \n",
      " ['con', 'much', 'alegr', 'entusiasm', 'dia', 'hoy', 'realiz', 'socializ', 'program', 'proyect', 'vincul', 'escuel', 'investig', 'agropecuari', 'agroindustrial', 'cont', 'particip', 'distint', 'comun', '1/2', 'somosyachaytech']\n",
      "Tagged tokens: \n",
      " [('con', 'NN'), ('much', 'JJ'), ('alegr', 'NN'), ('entusiasm', 'NN'), ('dia', 'NN'), ('hoy', 'NN'), ('realiz', 'NN'), ('socializ', 'JJ'), ('program', 'NN'), ('proyect', 'NN'), ('vincul', 'NN'), ('escuel', 'NN'), ('investig', 'NN'), ('agropecuari', 'VBP'), ('agroindustrial', 'JJ'), ('cont', 'NN'), ('particip', 'NN'), ('distint', 'NN'), ('comun', 'NN'), ('1/2', 'CD'), ('somosyachaytech', 'NN')]\n",
      "\n",
      "\n",
      "Tokens: \n",
      " ['Además', 'se', 'realizó', 'una', 'firma', 'de', 'convenio', 'y', 'carta', 'compromiso', 'con', '@', 'AgriculturaEc', 'y', 'la', 'Fundacion', '@', 'HeiferEcuador', ',', 'con', 'el', 'objetivo', 'de', 'establecer', 'mecanismos', 'de', 'cooperación', 'interinstitucional', 'y', 'desarollo', 'productivo', '.', '#', 'SomosYachayTech']\n",
      "Cleaned tokens and punctuation: \n",
      " ['ademas', 'realiz', 'firm', 'conveni', 'cart', 'compromis', 'agriculturaec', 'fundacion', 'heiferecu', 'objet', 'establec', 'mecan', 'cooper', 'interinstitucional', 'desaroll', 'product', 'somosyachaytech']\n",
      "Tagged tokens: \n",
      " [('ademas', 'NN'), ('realiz', 'NN'), ('firm', 'NN'), ('conveni', 'VBD'), ('cart', 'JJ'), ('compromis', 'NN'), ('agriculturaec', 'NN'), ('fundacion', 'NN'), ('heiferecu', 'NN'), ('objet', 'NN'), ('establec', 'NN'), ('mecan', 'JJ'), ('cooper', 'JJ'), ('interinstitucional', 'JJ'), ('desaroll', 'NN'), ('product', 'NN'), ('somosyachaytech', 'NN')]\n",
      "\n",
      "\n",
      "Cleaning dataset: \n",
      " ['la junt electoral yachay tech comun acuerd cronogram establec 05 18 agost realiz inscripcion candidatur somosyachaytech', 'yachay tech recib visit 40 estudi univers estatal peninsul sant elen upse_ec objet conoc instal laboratori equip dispon univers ademas ofert academ vigent posgr', 'con much alegr entusiasm dia hoy realiz socializ program proyect vincul escuel investig agropecuari agroindustrial cont particip distint comun 1/2 somosyachaytech', 'ademas realiz firm conveni cart compromis agriculturaec fundacion heiferecu objet establec mecan cooper interinstitucional desaroll product somosyachaytech']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "cleaning_dataset = []\n",
    "\n",
    "for text in df[0]:\n",
    "  tokens = word_tokenize(text)\n",
    "  print('Tokens: \\n',tokens)\n",
    "\n",
    "  cleaned_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stopword_es and token not in punctuation]\n",
    "  print('Cleaned tokens and punctuation: \\n',cleaned_tokens)\n",
    "\n",
    "  tagged = pos_tag(cleaned_tokens)\n",
    "  print('Tagged tokens: \\n',tagged)\n",
    "  print('\\n')\n",
    "\n",
    "  cleaning_dataset.append(' '.join(cleaned_tokens))\n",
    "\n",
    "print('Cleaning dataset: \\n',cleaning_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQKa5f7Mnq74"
   },
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wXnCp6bxnr0K",
    "outputId": "ca4d083d-abc3-45a0-f443-9bc49667e64e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   TF-IDF\n",
      "05               0.274192\n",
      "la               0.274192\n",
      "acuerd           0.274192\n",
      "junt             0.274192\n",
      "agost            0.274192\n",
      "inscripcion      0.274192\n",
      "18               0.274192\n",
      "electoral        0.274192\n",
      "candidatur       0.274192\n",
      "cronogram        0.274192\n",
      "comun            0.216176\n",
      "tech             0.216176\n",
      "establec         0.216176\n",
      "yachay           0.216176\n",
      "somosyachaytech  0.175013\n",
      "realiz           0.175013\n",
      "compromis        0.000000\n",
      "program          0.000000\n",
      "academ           0.000000\n",
      "laboratori       0.000000\n",
      "mecan            0.000000\n",
      "much             0.000000\n",
      "objet            0.000000\n",
      "ofert            0.000000\n",
      "particip         0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfIdf = tfIdfVectorizer.fit_transform(cleaning_dataset)\n",
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.head(25))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
